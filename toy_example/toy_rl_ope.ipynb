{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3cef73-151f-4b86-ab97-6e18db413a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.utils import seeding\n",
    "\n",
    "\n",
    "import math\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "\n",
    "from gymnasium import logger, spaces\n",
    "from gymnasium.envs.classic_control import utils\n",
    "from gymnasium.error import DependencyNotInstalled\n",
    "\n",
    "from os import path\n",
    "from scipy.optimize import minimize\n",
    "import d3rlpy\n",
    "from d3rlpy.dataset import MDPDataset\n",
    "\n",
    "from d3rlpy.metrics import EnvironmentEvaluator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class toy_env(gym.Env):\n",
    "    \"\"\"\n",
    "    1-D toy MDP:\n",
    "      S in [0,5], A in {0,1}.\n",
    "      Reward: r(s,a) = (26 - s^2 - I[a=1]) / 26  \\in [0,1].\n",
    "      Transition (with clipping to [0,5]):\n",
    "        if a=0: s' ~ Uniform( max(s-0.2, 0), min(s+1.0, 5) )\n",
    "        if a=1: s' ~ Uniform( max(0.2*s - 0.02, 0), min(s+0.5, 5) )\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_episode_steps: int = 100):\n",
    "        super().__init__()\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0.0], dtype=np.float32),\n",
    "            high=np.array([5.0], dtype=np.float32),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "        self.max_episode_steps = int(max_episode_steps)\n",
    "        self._elapsed_steps = 0\n",
    "        self.state: float | None = None\n",
    "\n",
    "        # rng\n",
    "        self.np_random, _ = seeding.np_random(None)\n",
    "\n",
    "    # ---------- helpers ----------\n",
    "    @staticmethod\n",
    "    def _reward(s: float, a: int) -> float:\n",
    "        return (26.0 - s**2 - (1.0 if a == 1 else 0.0)) / 26.0\n",
    "\n",
    "    def _sample_next_state(self, s: float, a: int) -> float:\n",
    "        if a == 0:\n",
    "            lo, hi = s - 0.2, s + 1.0\n",
    "        else:  # a == 1\n",
    "            lo, hi = 0.2 * s - 0.02, s + 0.5\n",
    "        lo = max(lo, 0.0)\n",
    "        hi = min(hi, 5.0)\n",
    "        return float(self.np_random.uniform(lo, hi) if hi > lo else lo)\n",
    "\n",
    "    # ---------- Gymnasium API ----------\n",
    "    def reset(self, *, seed: int | None = None, options: dict | None = None):\n",
    "        if seed is not None:\n",
    "            self.np_random, _ = seeding.np_random(seed)\n",
    "   \n",
    "        self.state = float(self.np_random.uniform(2.0, 3.0))\n",
    "        self._elapsed_steps = 0\n",
    "        obs = np.array([self.state], dtype=np.float32)\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action: int):\n",
    "        assert self.state is not None, \"Call reset() before step().\"\n",
    "        a = int(action)\n",
    "        if not self.action_space.contains(a):\n",
    "            raise gym.error.InvalidAction(f\"Invalid action {action!r}\")\n",
    "\n",
    "        s = self.state\n",
    "        r = self._reward(s, a)\n",
    "        s_next = self._sample_next_state(s, a)\n",
    "\n",
    "        self.state = s_next\n",
    "        self._elapsed_steps += 1\n",
    "\n",
    "        obs = np.array([s_next], dtype=np.float32)\n",
    "        terminated = False  # no absorbing terminal\n",
    "        truncated = self._elapsed_steps >= self.max_episode_steps\n",
    "        info = {}\n",
    "        return obs, float(r), terminated, truncated, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4b9342f-69b3-427d-b2b6-61d5e7102689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_random_policy(state):\n",
    "    return np.random.randint(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a17db65b-860b-4e99-99da-48e91c5d6319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generation(EnvClass, policy, num_episodes = 100, seed_da = 10):\n",
    "    \n",
    "    env = EnvClass()\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    terminals = []\n",
    "\n",
    "    for tt in range(num_episodes):\n",
    "        obs = env.reset(seed = seed_da+100*tt)[0]  # Reset the environment\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = policy(obs)  # Use a random policy\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Store the transition\n",
    "            states.append(obs)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            terminals.append(done)\n",
    "\n",
    "            obs = next_obs  # Update current state\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    states = np.array(states, dtype=np.float32)\n",
    "    actions = np.array(actions, dtype=np.int32)\n",
    "    rewards = np.array(rewards, dtype=np.float32)\n",
    "    terminals = np.array(terminals, dtype=np.bool_)\n",
    "\n",
    "    dataset = MDPDataset(states, actions, rewards, terminals)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_policy(policy, EnvClass, gamma=0.95, num_episodes = 100, seed = 20):\n",
    "\n",
    "    env = EnvClass()\n",
    "    rewards = []\n",
    "\n",
    "    for tt in range(num_episodes):\n",
    "        obs, _ = env.reset(seed = seed + tt*100)\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        discount = 1.0\n",
    "\n",
    "        while not done:\n",
    "            # Select action from the policy\n",
    "            action = policy.sample_action(obs.reshape(1,-1)).item()\n",
    "\n",
    "            # Step in the environment\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Accumulate discounted reward\n",
    "            episode_reward += discount * reward\n",
    "            discount *= gamma\n",
    "\n",
    "            # Update observation\n",
    "            obs = next_obs\n",
    "\n",
    "        rewards.append(episode_reward)\n",
    "\n",
    "    return np.mean(rewards) # Mean and standard deviation of rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24a9a645-8932-463e-b3d9-6241ddcfbee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sac(MDP_data, env, gamma = 0.95):\n",
    "    \n",
    "    sac_beh = d3rlpy.algos.DiscreteSACConfig(gamma=gamma,initial_temperature=0.06,temp_learning_rate=0).create(device=\"cuda:0\")\n",
    "    sac_beh.build_with_dataset(MDP_data)\n",
    "    env_evaluator = EnvironmentEvaluator(env)\n",
    "    sac_beh.fit(MDP_data,\n",
    "        n_steps=20000,\n",
    "        n_steps_per_epoch=2000,\n",
    "        show_progress=False,\n",
    "        evaluators={\n",
    "            'environment': env_evaluator,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return sac_beh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e00257c-e070-4b7b-9b71-cef3764b7e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "MDP_data = data_generation(EnvClass = toy_env, policy = custom_random_policy, num_episodes = 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc8a2bf-3220-4152-a9fa-780659cad29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d3rlpy.seed(123)\n",
    "sac_beh = train_sac(MDP_data, env = toy_env(), gamma = 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "073e6cae-448e-4783-940f-fca95bdb8120",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac_beh.save(\"sac_model_toy.d3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f036eac-83b9-48e8-af16-025c31bf3bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3d97d7-6610-420c-ae9f-2464f451e38c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ac301b-d4ff-4ca2-a8bf-ab2372bb3722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8d88a9-6ee7-415b-9fc3-767ab8af2c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import d3rlpy\n",
    "import gym                 # old gym library\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "\n",
    "class toy_env_gym(gym.Env):\n",
    "    \"\"\"\n",
    "    1-D toy MDP (old gym edition that works for scope-rl):\n",
    "      S in [0,5], A in {0,1}.\n",
    "      Reward: r(s,a) = (30 - s^2 - 5 * I[a=1]) / 30 âˆˆ [0,1].\n",
    "      Transition (clipped to [0,5]):\n",
    "        if a=0: s' ~ Uniform(max(s-0.2, 0), min(s+1.0, 5))\n",
    "        if a=1: s' ~ Uniform(max(0.2*s-0.02, 0), min(s+0.5, 5))\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {}  \n",
    "\n",
    "    def __init__(self, max_episode_steps: int = 100):\n",
    "        super().__init__()\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0.0], dtype=np.float32),\n",
    "            high=np.array([5.0], dtype=np.float32),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "        self.max_episode_steps = int(max_episode_steps)\n",
    "        self._elapsed_steps = 0\n",
    "        self.state = None\n",
    "\n",
    "  \n",
    "        self.np_random, _ = seeding.np_random(None)\n",
    "\n",
    "    # ---------- helpers ----------\n",
    "    @staticmethod\n",
    "    def _reward(s: float, a: int) -> float:\n",
    "        return (30.0 - s**2 - 5.0 * (1.0 if a == 1 else 0.0)) / 30.0\n",
    "\n",
    "    def _sample_next_state(self, s: float, a: int) -> float:\n",
    "        if a == 0:\n",
    "            lo, hi = s - 0.2, s + 1.0\n",
    "        else:  # a == 1\n",
    "            lo, hi = 0.2 * s - 0.02, s + 0.5\n",
    "        lo = max(lo, 0.0)\n",
    "        hi = min(hi, 5.0)\n",
    "        return float(self.np_random.uniform(lo, hi) if hi > lo else lo)\n",
    "\n",
    " \n",
    "    def reset(self, *, seed: int | None = None, options: dict | None = None):\n",
    "        if seed is not None:\n",
    "            self.np_random, _ = seeding.np_random(seed)\n",
    "        \n",
    "        self.state = float(self.np_random.uniform(2.0, 3.0))\n",
    "        self._elapsed_steps = 0\n",
    "        obs = np.array([self.state], dtype=np.float32)\n",
    "        info = {}\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action: int):\n",
    "        if not self.action_space.contains(action):\n",
    "            raise gym.error.InvalidAction(f\"Invalid action {action!r}\")\n",
    "\n",
    "        s = float(self.state)\n",
    "        a = int(action)\n",
    "\n",
    "        r = self._reward(s, a)\n",
    "        s_next = self._sample_next_state(s, a)\n",
    "\n",
    "        self.state = s_next\n",
    "        self._elapsed_steps += 1\n",
    "\n",
    "        obs = np.array([s_next], dtype=np.float32)\n",
    "        terminated = False                    \n",
    "        truncated = self._elapsed_steps >= self.max_episode_steps\n",
    "        info = {}\n",
    "        return obs, float(r), terminated, truncated, info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ea8ce02-7f6a-4a8f-a9a1-cf4c1461de4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary module from SCOPE-RL\n",
    "from scope_rl.dataset import SyntheticDataset\n",
    "from scope_rl.policy import EpsilonGreedyHead\n",
    "from scope_rl.ope import CreateOPEInput\n",
    "from scope_rl.ope import OffPolicyEvaluation as OPE\n",
    "from scope_rl.ope.discrete import TrajectoryWiseImportanceSampling as TIS        \n",
    "from scope_rl.ope.discrete import PerDecisionImportanceSampling as PDIS \n",
    "from scope_rl.ope.discrete import DirectMethod as DM\n",
    "from scope_rl.ope.discrete import DoublyRobust as DR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2db6f35f-14d7-4c6a-9ec9-8c0840344618",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_env_1 = toy_env_gym()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c181dff-dc82-49a1-b00b-40a3075cf45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "behavior_policy = EpsilonGreedyHead(\n",
    "    sac_beh,\n",
    "    n_actions=2,\n",
    "    epsilon=1.0,\n",
    "    name=\"sac_beh\",\n",
    "    random_state=12345,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a551a5-df76-4490-8c32-273c45cf7455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# create input for off-policy evaluation (OPE)\\nprep = CreateOPEInput(\\n    env=toy_env_1,\\n    gamma=0.95,\\n)\\ninput_dict = prep.obtain_whole_inputs(\\n    logged_dataset=logged_dataset,\\n    evaluation_policies=[sac_policy],\\n    require_value_prediction=True,\\n    require_weight_prediction=False,\\n    q_function_method = \"fqe\",\\n    w_function_method = \"mwl\",\\n    n_steps=10000,\\n    n_trajectories_on_policy_evaluation=100,\\n    random_state=123,\\n)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sac_policy = EpsilonGreedyHead(\n",
    "    sac_beh,\n",
    "    n_actions=2,\n",
    "    epsilon=0.0,\n",
    "    name=\"sac_beh\",\n",
    "    random_state=12345,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182749ee-81ae-4a5b-b0b9-8d1647a02b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d3rlpy.seed(2)\n",
    "np.random.seed(2)\n",
    "kkk = 10\n",
    "ope_result = np.zeros((kkk, 3))\n",
    "\n",
    "for i in range(kkk):\n",
    "    # initialize dataset class\n",
    "    dataset = SyntheticDataset(\n",
    "        env=toy_env_1,\n",
    "        max_episode_steps=100,\n",
    "    )\n",
    "    \n",
    "    # data collection\n",
    "    logged_dataset = dataset.obtain_episodes(\n",
    "        behavior_policies=behavior_policy,\n",
    "        n_trajectories=200,\n",
    "        random_state=i,\n",
    "    )\n",
    "    \n",
    "    prep = CreateOPEInput(\n",
    "        env=toy_env_1,\n",
    "        gamma=0.95,\n",
    "    )\n",
    "    input_dict = prep.obtain_whole_inputs(\n",
    "        logged_dataset=logged_dataset,\n",
    "        evaluation_policies=[sac_policy],\n",
    "        require_value_prediction=True,\n",
    "        n_steps=20000,\n",
    "        n_trajectories_on_policy_evaluation=100,\n",
    "        random_state=i*20,\n",
    "    )\n",
    "    \n",
    "    ope = OPE(\n",
    "        logged_dataset=logged_dataset,\n",
    "        ope_estimators=[DM(), DR(),PDIS()],\n",
    "    )\n",
    "    policy_value_dict = ope.estimate_policy_value(\n",
    "        input_dict=input_dict,\n",
    "    )\n",
    "\n",
    "    ope_result[i,0] = policy_value_dict[\"sac_beh\"][\"dm\"]\n",
    "    ope_result[i,1] = policy_value_dict[\"sac_beh\"][\"dr\"]\n",
    "    ope_result[i,2] = policy_value_dict[\"sac_beh\"][\"pdis\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bf9bf1-5151-478a-b5d1-54db9de4d1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"toy_ope_result.csv\", ope_result, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318859c1-bdbe-4f24-a547-b7a891748587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "wass_result = pd.read_csv(\"wass_result.csv\", index_col=0)\n",
    "wass_result = wass_result.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1050c5bf-03c7-41a5-9d18-9f564fefaee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wass_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c52cf6-10fb-43ff-b236-489b3f1bc697",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tv_result = pd.read_csv(\"tv_result.csv\", index_col=0)\n",
    "\n",
    "tv_result = tv_result.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139f2077-7d8e-4f12-ae66-91a9950ed478",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c576e6-2162-487c-b013-30accb83277a",
   "metadata": {},
   "outputs": [],
   "source": [
    "est_value_abs = pd.read_csv(\"est_value_abs.csv\", index_col=0)\n",
    "est_value_abs = est_value_abs[\"x\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf24d98a-f141-4813-8e33-28e0227d6c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "def plot_ope_summary(wass_result, tv_result, ope_result, true_vals,show=True):\n",
    "    x   = np.array([0, 0.2, 0.4, 0.6, 0.8, 1.0], dtype=float)\n",
    "    z   = 1.96\n",
    "    eps = 0.04\n",
    "\n",
    "    def mean_ci(arr_2d):\n",
    "        m = np.mean(arr_2d, axis=0)\n",
    "        s = np.std(arr_2d, axis=0, ddof=1)\n",
    "        n = arr_2d.shape[0]\n",
    "        half = z * s / np.sqrt(n)\n",
    "        return m, m - half, m + half\n",
    "\n",
    "    def mean_ci_singlecol(col_1d, repeat_len=6):\n",
    "        m = float(np.mean(col_1d)); s = float(np.std(col_1d, ddof=1)); n = len(col_1d)\n",
    "        half = z * s / np.sqrt(n)\n",
    "        return np.full(repeat_len, m), np.full(repeat_len, m-half), np.full(repeat_len, m+half)\n",
    "\n",
    "    wass_mean, wass_lo, wass_hi = mean_ci(wass_result)\n",
    "    tv_mean,   tv_lo,   tv_hi   = mean_ci(tv_result)\n",
    "    dm_mean,   dm_lo,   dm_hi   = mean_ci_singlecol(ope_result[:, 0])\n",
    "    dr_mean,   dr_lo,   dr_hi   = mean_ci_singlecol(ope_result[:, 1])\n",
    "    #pdis_mean, pdis_lo, pdis_hi = mean_ci_singlecol(ope_result[:, 2])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5), layout='constrained')\n",
    "\n",
    "    ax.fill_between(x, wass_lo, wass_hi, alpha=0.2, color=\"skyblue\")\n",
    "    ax.plot(x, wass_mean, marker=\"o\", color=\"steelblue\", label=\"Wass Method\")\n",
    "\n",
    "    ax.fill_between(x, tv_lo, tv_hi, alpha=0.2, color=\"orange\")\n",
    "    ax.plot(x, tv_mean, marker=\"o\", color=\"darkorange\", label=\"TV Method\")\n",
    "\n",
    "    ax.fill_between(x, dm_lo, dm_hi, alpha=0.2, color=\"lightgreen\")\n",
    "    ax.plot(x, dm_mean, marker=\"o\", color=\"green\", label=\"DM\")\n",
    "\n",
    "    ax.fill_between(x, dr_lo, dr_hi, alpha=0.2, color=\"plum\")\n",
    "    ax.plot(x, dr_mean, marker=\"o\", color=\"purple\", label=\"DR\")\n",
    "\n",
    "    #ax.fill_between(x, pdis_lo, pdis_hi, alpha=0.2, color=\"thistle\")\n",
    "    #ax.plot(x, pdis_mean, marker=\"o\", color=\"indigo\", label=\"PDIS\")\n",
    "\n",
    "    for xi, yi in zip(x, true_vals):\n",
    "        ax.plot([xi - 0.04, xi + 0.04], [yi, yi], color=\"red\", linewidth=3)\n",
    "    ax.plot([], [], color=\"red\", linewidth=3, label=\"True Value\")\n",
    "\n",
    "    ax.set_xlabel(\"Wass Dist\")\n",
    "    ax.set_ylabel(\"Value\")\n",
    "    #ax.set_title(\"Worst-case OPE with different Wass Perturbation\")\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.3)\n",
    "    ax.legend(loc=\"center left\", bbox_to_anchor=(1.02, 0.5), borderaxespad=0.)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4ee2e5-2df0-4b1a-a4ea-51b603288d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_ope_summary(wass_result= wass_result,\n",
    "                     tv_result = tv_result,\n",
    "                     ope_result = ope_result,   # shape (n_sims, 3) -> [DM, DR, PDIS]\n",
    "                     true_vals = est_value_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc3c4f1-3dc2-4860-982f-a857b558edb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"ope_plot.pdf\", dpi=300, bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (UIUC User)",
   "language": "python",
   "name": "uiuc-user-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
